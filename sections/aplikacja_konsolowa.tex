Aplikacja konsolowa powstała w celu przetwarzania czasochłonnych zadań w tle, tak by użytkownik aplikacji webowej nie doświadczał długich czasów ładowania oraz ewentualnych błędów podczas przerwania sesji lub połączenia internetowego ze stroną. Aplikacja konsolowa uruchamiana jest co 5 minut i przetwarza zadania trzech typów w kolejności widocznej na rysunku \ref{fig:worker_proces}. Poszczególne procesy zostały szerzej opisane w kolejnych podrozdziałach.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{worker_proces.png}
	\caption{Proces działania aplikacji konsolowej}
	\label{fig:worker_proces}
\end{figure}

\section{Technologie}
Aplikacja konsolowa powstała w języku programowania Python w wersji trzeciej. W początkowej fazie projektu za wyborem tego języka przemawiała wieloplatformowość, możliwości wprowadzania szybkich zmian w kodzie i brak potrzeby go kompilowania. C\#, który został wybrany do stworzenia aplikacji webowej okazał się nie przystosowany do uruchomienia na platformie Raspberry Pi z powodu braku dostępnego .NET Core SDK na procesory ARM oraz wymaganiu znacznie większej ilości zasobów niż Python. Ostatecznie aplikacja konsolowa została przeniesiona na zewnętrzny serwer, ale popularność Pythona pozwoliła na szybką integrację z usługami firm trzecich, dzięki przygotowanym przez firmy paczki deweloperskie.

\section{Proces wykrywania twarzy}
Uogólniony proces detekcji twarzy na obrazie został przedstawiony na grafie \ref{fig:wykrywanie_proces}.
Proces przetwarzania zadań detekcji rozpoczyna się od pobrania z bazy wszystkich żądań o statusie 'New'. Następnie każde zadanie przetwarzane jest osobno. Dla aktualnie procesowanego zadania pobierany jest obraz wejściowy z usługi Dropbox i zapisywany w lokalnym folderze. Następnym krokiem jest wywołanie procesu odpowiedzialnego za odpowiednie przygotowanie zdjęcia, a następnie pozyskanie oczekiwanych wyników, w sposób odpowiedni dla danego sposobu. Krok ten został szerzej opisany w podrozdziałach \ref{detekcja_haar}, \ref{detekcja_dnn} i \ref{detekcja_azure}. po uzyskaniu wyników każdą dostępną w programie metodą, zostaje utworzony obraz wyjściowy dla każdej metody (haar, azure, ..), które zostaje wgrany do odpowiedniego folderu w usłudze Dropbox. Po poprawnym wgraniu plików wynikowych informacja o rezultatach zostaje dodana do bazy danych. Po wykonaniu zadania bez żadnych błędów, request zostaje oznaczony jako zakończony, w innym przypadku zostaje nadany mu status 'Error'. Proces powtarzany jest dla każdego wpisu pozyskanego z bazy.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{wykrywanie_twarzy.png}
	\caption{Proces wykrywania twarzy zaimplementowany w aplikacji konsolowej}
	\label{fig:wykrywanie_proces}
\end{figure}

\subsection{OpenCv Haar} \label{detekcja_haar}
Pierwszą z metod detekcji twarz, która została zintegrowana z programem jest detekcja metodą Haar'a. Algorytm został opisany w rozdziale \ref{haar}. Przed uruchomieniem detekcji twarzy, obraz wejściowy należy odpowiednio przygotować. W tym celu wcześniej pobrany obraz zostaje wczytany do programu i przekonwertowany do odcieni szarości. Tak przygotowany obraz można poddać detekcji. Detektor zwraca współrzędne obszarów zawierających w sobie twarze. Tak uzyskane dane należy przekonwertować do formatu, który został przyjęty jako wspólny dla wszystkich metod.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{detekcja_haar.png}
	\caption{Wykrywanie twarzy metodą Haar zaimplementowane w aplikacji konsolowej}
	\label{fig:wykrywanie_haar}
\end{figure}

\subsection{OpenCv DNN (Deep Neural Network)} \label{detekcja_dnn}
Proces detekcji z wykorzystaniem głęboko uczonej sieci neuronowej nie wymaga formatowania obrazu do skali szarości, ale za to obraz musi zostać przeskalowany do odpowiedniego, wcześniej przyjętego rozmiaru. Dodatkową zaletą tej metody jest format odpowiedzi detektora, który oprócz obszaru zawierającego twarz zwraca również 'pewność' z jaką twarz została wykryta, co pozwala na odfiltrowanie niezadowalających użytkownika wyników.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{detekcja_dnn.png}
	\caption{Wykrywanie twarzy metodą DNN (Deep Neural Network) zaimplementowane w aplikacji konsolowej}
	\label{fig:wykrywanie_dnn}
\end{figure}

\subsection{Azure Cognitive Services} \label{detekcja_azure}
Kolejna wykorzystana metoda detekcji twarzy, korzysta z usługi dostępnej na platformie Azure, dzięki czemu środowisko rozruchowe zostaje odciążone z obliczeń związanych z przetwarzaniem obrazu. Proces wykrycia twarzy ogranicza się do przesłania wybranego obrazu do Cognitive Services Api. W odpowiedzi klient uzyskuje JSON zawierający parametry, które podano jako wymagane podczas wykonywania zapytania. JSON zostaje zparsowany na obiekt, którego wartości zostają przekonwertowane do wymaganego formatu [startX, startY, endX, endY].
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{detekcja_azure.png}
	\caption{Wykrywanie twarzy używając Azure Cognitive Services zaimplementowane w aplikacji konsolowej}
	\label{fig:wykrywanie_azure}
\end{figure}


\section{Proces trenowania modelu dla wybranych algorytmów \textcolor{red}{cos o tej sieci napisac lub omowic jej zrodla}}
Proces trenowania sieci został przedstawiono na grafie \ref{fig:trenowanie_proces}. Po jego zakończeniu użytkownik otrzymuje grupę wytrenowanych modeli zawierającą gotowe do użycia metody rozpoznawania twarzy przedstawione w rozdziale \ref{s:open_cv} oraz \ref{azurecs}.
Proces zaczyna się od sprawdzenia jakie dane uczące (profile utworzone w zakładce 'People' aplikacji webowej) są dostępne lokalnie. Repozytorium zostaje porównane z danymi dostępny w bazie, a następnie zostają pobrane zdjęcia wszystkich brakujących osób.

Kolejnym krokiem jest pobrania z bazy wszystkich nowych grup algorytmów rozpoznawania twarzy, które nie zostały jeszcze zakończone. Następnie dla każdego zgłoszenia z listy wykonywany jest proces przygotowania danych uczących na podstawie zdjęć osób, które wybrano podczas tworzenia zadania w aplikacji webowej. Po odpowiednim przygotowaniu danych grupa sieci zostaje nauczona i dla metod tego wymagających zostaje utworzony plik w formacie xml przechowujący wytrenowany model. Taki plik zostaje zapisany w bazie danych oraz wysłany do odpowiedniego folderu w Dropboxie.

W przypadku braku komplikacji, request zostaje oznaczony jako zakończony. Proces zostaje powtórzony dla każdego zadania wczytanego do listy na początku grafu \ref{fig:trenowanie_proces}. Sposób przygotowania danych oraz nauczania dla poszczególnych metod został opisany w kolejnych podrozdziałach.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{proces_nauczania.png}
	\caption{Proces trenowania sieci zaimplementowany w aplikacji konsolowej}
	\label{fig:trenowanie_proces}
\end{figure}

\subsection{Trenowanie identyfikatorów OpenCv} \label{trenowanie_open_cv}
Ogólny schemat procesu trenowania identyfikatorów twarzy OpenCv (Eigenfaces, Fisherfaces, LHGP) został przedstawiony na rysunku \ref{fig:trenowanie_open_cv}. Zalecany proces trenowania udostępniony w \fnurl{dokumentacji}{https://docs.opencv.org/3.4.3/da/d60/tutorial\_face\_main.html\#tutorial\_face\_conclusion} został poddany zmianą wymaganym przez aplikację konsolową i zaimplementowany.

Proces rozpoczyna się od stworzenia listy zawierającej wszystkie przypisane do sieci profile oraz załączone do nich obrazy. Następnie dla każdego zdjęcia z listy zostaje wykonany preprocessing. Pierwszym krokiem jest próba wykrycia twarzy na obrazie. W przypadku nie znalezienia twarzy, obraz zostaje zignorowany i program przechodzi do przetwarzania kolejnego obrazu. Jeśli na zdjęciu zostanie zlokalizowana twarz, to obszar ją zawierający zostaje przeskalowany do odcieni szarości, a następnie przekonwertowany do postaci numpy array. Wektor zawierający informację o twarzy zostaje dodany do listy uczącej wraz z odpowiadającym mu ID profilu, do którego należy zdjęcie. Po przetworzeniu wszystkich obrazów na liście, zostaje utworzone i nauczone wszystkie wcześniej wymienione identyfikatory dostępne w OpenCv.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{trenowanie_open_cv.png}
	\caption{Trenowanie algorytmów z biblioteki OpenCv zaimplementowane w aplikacji konsolowej}
	\label{fig:trenowanie_open_cv}
\end{figure}

\subsection{Trenowanie identyfikatora Azure} \label{trenowanie_azure}
Proces rozpoczyna się od wczytania wszystkich osób oraz ich zdjęć, które zostały przypisane do zadania trenowania sieci. Na diagramie \ref{fig:trenowanie_azure} niebieskim kolorem oznaczono zapytania do Azure Cognitive Services API(Application Programming Interface).

Zgodnie z \fnurl{dokumentacją}{https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/Quickstarts/python-analyze} pierwszym i najważniejszym krokiem jest utworzenie nowej AzureLargeGroup, do której w kolejnym kroku zostaną dodane id/nazwy wszystkich profili znajdujących się na wczytanej liście.  Następnie każdy obraz z listy zostaje przypisany odpowiedniej osobie w AzureLargeGroup. W tym rozwiązaniu klient nie musi martwić się detekcją twarzy na obrazie, bo jest ona wykonywana przez usługę Azure po przesłaniu obrazu. W przypadku problemów z wykryciem twarzy na obrazie, zostanie zwrócona informacja mówiąca o tym że plik zostanie zignorowany podczas procesu nauczania. Po dodaniu wszystkich zdjęć, program wywołuje funkcję trenowania nowej sieci na podstawie danych dołączonych do AzureLargeGroup. Już po około sekundzie, identyfikator tożsamości udostępniony przez usługę Azure jest gotowy do działania.

Ze względu na setki zapytań, które muszą zostać wykonane do zewnętrznego serwisu Azure, proces przygotowania danych przed trenowaniem może wydłużyć się nawet do kilku minut w przypadku wybrania dużej ilości profili i zdjęć.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{trenowanie_azure.png}
	\caption{Trenowanie usługi Azure Cognitive Services zaimplementowane w aplikacji konsolowej}
	\label{fig:trenowanie_azure}
\end{figure}

\section{Proces rozpoznawania twarzy}
Ogólny proces rozpoznawania twarzy przedstawiony na rysunku \ref{fig:rozpoznawanie_proces} jest zbliżony do wcześniej opisywanych procesów detekcji i trenowania. 

Proces rozpoczyna się od pobrania wszystkich wytrenowanych modeli, których aktualnie nie ma w lokalnym katalogu. W kolejnym etapie zostają wczytane wszystkie nowe zadania rozpoznania twarzy. Dla każdego z requestów na liście wykonywane są te same czynności, a pierwszą z nich jest pobranie pliku wejściowego z usługi Dropbox. Następnie twarz zostaje rozpoznana za pomocą każdego dostępnego algorytmu identyfikacji. 

Poszczególne metody zostały szerzej opisane w kolejnych podrozdziałach. Po ukończeniu identyfikacji tożsamości, wszystkie uzyskane wyniki zostają wprowadzone do bazy danych, a zadanie oznaczone jako ukończone. Gdy zadania na liście skończą się to dochodzi do zakończenia procesu.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{rozpoznawanie_twarzy.png}
	\caption{Proces identyfikacji tożsamości zaimplementowany w aplikacji konsolowej}
	\label{fig:rozpoznawanie_proces}
\end{figure}

\subsection{Identyfikacja tożsamości metodami dostępnymi w OpenCv}
Niezależnie od typu FaceRecognizera, proces rozpoznawania twarzy z użyciem algorytmów OpenCv wygląda identycznie. Przykładowe zastosowanie zostało opisane w \fnurl{dokumentacji}{https://docs.opencv.org/3.4.3/da/d60/tutorial\_face\_main.html\#tutorial\_face\_conclusion}. 

Obraz zostaje wczytany do programu, a następnie zostaje podjęta próba detekcji twarzy metodą Haara. W przypadku nie wykrycia żadnej twarzy do zadania zostaje dodany pusty rezultat z komentarzem informującej o braku twarzy na przekazanym obrazie. 
Jeśli na zdjęciu zostanie wykryta twarz to proces przechodzi do kolejnego kroku, którym jest wczytanie modelu odpowiadającego typowi identyfikatora i numerowi wybranej grupy sieci. Przed identyfikacją wycinek obrazu zawierający twarz zostaje przekonwertowany do numpy array, który jest formatem wymaganym przez FaceRecognizer. W rezultacie uzyskana zostaje tożsamość osoby ze zdjęcia prezentowana w postaci numeru Id z bazy oraz wartość wektora oddalenia od najbliższego obiektu w modelu (0.0 oznacza idealne dopasowanie, im większa wartość tym mniejsza pewność). 
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{rozpoznawanie_open_cv.png}
	\caption{Proces identyfikacji osoby wykorzystując identyfikatory OpenCv zaimplementowany w aplikacji konsolowej}
	\label{fig:rozpoznawanie_open_cv}
\end{figure}

\subsection{Identyfikacja tożsamości przy użyciu Azure Cognitive Services}
Proces rozpoznawania twarzy przy pomocy Azure Cognitive Services jest mniej obciążający dla platformy, na której uruchomiono program, ale za to może być wolniejszy od pozostałych rozwiązań ze względu na ograniczenia jakości połączenia.

Kroki procesu identyfikacji zaimplementowano zgodnie z \fnurl{dokumentacją}{https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/Quickstarts/python-analyze},a pierwszym z nich jest wczytanie słownika tożsamości przypisanych do AzureLargeGroup. W kolejnym etapie zdjęcie wejściowe zostaje wysłane do usługi detekcji opisanej w \ref{detekcja_azure} ale ze zmodyfikowanymi parametrami. Każdej wykrytej twarzy zostaje nadany numer identyfikacyjny Azure. 

Jeśli jakaś twarz została znaleziona to kolejnym etapem jest wysłanie żądania identyfikacji uzyskanej twarzy przy pomocy AzureLargeGroup, która została przypisana do tego requesta. Tożsamość rozpoznana przez CS zostaje zwrócona w postaci numera identyfikacyjnego usługi, dlatego musi ona zostać porównana z wcześniej wczytanym słownikiem, który pozwala na odczytanie Id osoby istniejącej w bazie danych programu.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{rozpoznawanie_azure.png}
	\caption{Proces identyfikacji osoby wykorzystując Azure Cognitive Servces zaimplementowany w aplikacji konsolowej}
	\label{fig:rozpoznawanie_azure}
\end{figure}
